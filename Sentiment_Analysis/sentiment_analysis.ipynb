{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps for sentiments\n",
    "\n",
    "### Training\n",
    "1. Message is process to good and bad manually\n",
    "2. Train on whether the statement is good or bad\n",
    "\n",
    "\n",
    "### Prod\n",
    "1. naive bayes to classify message into good or bad\n",
    "2. Use K-means to classify the words\n",
    "    1. Use spiderweb method to determine amount of cluster\n",
    "    2. Create wordcloud of the words provided"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting and \n",
    "Seperating the message for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = pd.read_csv('ratingMessages', sep = '\\t', names = [\"rating\", \"message\"])\n",
    "\n",
    "\n",
    "messages = messages.sample(frac = 1, random_state = 0).reset_index(drop = True) ## shuffle and sample message\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To split the message and rating into training, validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = list(messages.message) \n",
    "lbls =list(messages.rating) \n",
    "trainingMsgs = msgs[:2500] \n",
    "valMsgs = msgs[2500:3500] \n",
    "testingMsgs = msgs[3500:]\n",
    "\n",
    "trainingLbls = lbls[:2500] \n",
    "valLbls = lbls[2500:3500]\n",
    "testingLbls = lbls[3500:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesForGoodRating:\n",
    "    def train (self, badMessages, goodMessages):\n",
    "        self.words = set (' '.join (badMessages + goodMessages).split())\n",
    "        self.priors = np.zeros (2)\n",
    "        self.priors[0] = float (len (badMessages)) / (len (badMessages) + len (goodMessages))\n",
    "        self.priors[1] = 1.0 - self.priors[0]\n",
    "        self.likelihoods = []\n",
    "        for i, w in enumerate (self.words):\n",
    "            prob1 = (1.0 + len ([m for m in badMessages if w in m])) / len (badMessages)\n",
    "            prob2 = (1.0 + len ([m for m in goodMessages if w in m])) / len (goodMessages)\n",
    "            self.likelihoods.append ([min (prob1, 0.95), min (prob2, 0.95)])\n",
    "        self.likelihoods = np.array (self.likelihoods).T\n",
    "        \n",
    "    def predict (self, message):\n",
    "        posteriors = np.copy (self.priors)\n",
    "        for i, w in enumerate (self.words):\n",
    "            if w in message.lower():  # convert to lower-case\n",
    "                posteriors *= self.likelihoods[:,i]\n",
    "            else:                                   \n",
    "                posteriors *= np.ones (2) - self.likelihoods[:,i]\n",
    "            posteriors = posteriors / np.linalg.norm (posteriors)  # normalise\n",
    "        if posteriors[0] > 0.5:\n",
    "            return ['bad', posteriors[0]]\n",
    "        return ['good', posteriors[1]]    \n",
    "\n",
    "    def score (self, messages, labels):\n",
    "        confusion = np.zeros(4).reshape (2,2)\n",
    "        for m, l in zip (messages, labels):\n",
    "            if self.predict(m)[0] == 'bad' and l == 'bad':\n",
    "                confusion[0,0] += 1\n",
    "            elif self.predict(m)[0] == 'bad' and l == 'good':\n",
    "                confusion[0,1] += 1\n",
    "            elif self.predict(m)[0] == 'good' and l == 'bad':\n",
    "                confusion[1,0] += 1\n",
    "            elif self.predict(m)[0] == 'good' and l == 'good':\n",
    "                confusion[1,1] += 1\n",
    "        return (confusion[0,0] + confusion[1,1]) / float (confusion.sum()), confusion\n",
    "\n",
    "    def predict_prod(self,messages):\n",
    "        result = []\n",
    "        for message in messages:\n",
    "            posteriors = np.copy (self.priors)\n",
    "            for i, w in enumerate (self.words):\n",
    "                if w in message.lower():  # convert to lower-case\n",
    "                    posteriors *= self.likelihoods[:,i]\n",
    "                else:                                   \n",
    "                    posteriors *= np.ones (2) - self.likelihoods[:,i]\n",
    "                posteriors = posteriors / np.linalg.norm (posteriors)  # normalise\n",
    "            if posteriors[0] > 0.5:\n",
    "                result.append('bad')\n",
    "            else:\n",
    "                result.append('good')\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "badmsg = [m for (m, l) in zip(trainingMsgs, trainingLbls) if 'bad' in l]\n",
    "goodmsg = [m for (m, l) in zip(trainingMsgs, trainingLbls) if 'good' in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = NaiveBayesForGoodRating()\n",
    "clf.train(badmsg, goodmsg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, confusion = clf.score (valMsgs, valLbls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The overall performance is:\", score)\n",
    "print(\"The confusion matrix is:\\n\", confusion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prod"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group messages using model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c715c4de3f1ed64850d6af484bd5fcd0f01891dd4db9740707a589aaabd33741"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
